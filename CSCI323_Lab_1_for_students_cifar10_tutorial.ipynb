{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddevMetal/CSCI218/blob/main/CSCI323_Lab_1_for_students_cifar10_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSCI323 Lab 1 Assignment (2025)\n",
        "**Content modified by: Cher Lim** (cherl@uow.edu.au)"
      ],
      "metadata": {
        "id": "m_yPxWucycfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name:**\n",
        "\n",
        "**7-digit UOW ID**:\n",
        "\n",
        "##Instructions:\n",
        "1. There are three sections (A, B, C) in this notebook.\n",
        "2. Run the code per section and review the computation results.\n",
        "3. Answer the questions **supported by your computation results**. Provide **explanation** to your answers to demonstrate your understanding of the relevant concepts.  \n",
        "4. You will print the entire notebook (answers, computation results) into **PDF and submit to Moodle**."
      ],
      "metadata": {
        "id": "d7AlYZ0C2qI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Exploring the dataset (2 marks)\n",
        "Review the t-SNE visualization outputs in Section A.\n",
        "\n",
        "1. List 4 pairs of classes that are heavily overlapped. (1 mark)\n",
        "2. The t-SNE visualization gives us a 'starting point' view of our data. What can you implement in a basic CNN model to better separate these overlapping classes? Give 5 suggestions. (1 mark)\n",
        "\n"
      ],
      "metadata": {
        "id": "aB_Ek5Iyg2jV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write your answers here:"
      ],
      "metadata": {
        "id": "1LBC5mHcKKyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###B. Training the model (2 marks)\n",
        "You are provided with a dataset of N samples. You split it into training and validation set with a ratio of 70:30.\n",
        "You then set the batch_size = k for your training.\n",
        "\n",
        "1. How many times does the model update its weights? (1 mark)\n",
        "2. How does batch size affect the model performance? (1 mark)"
      ],
      "metadata": {
        "id": "umC-rcZriFGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write your answers here:"
      ],
      "metadata": {
        "id": "da0WQ-IhKPl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Role of activation function (6 marks)\n",
        "You will train 3 CNN models:\n",
        "* A standard CNN model with no activation function at the output layer.\n",
        "* A non-standard CNN model with ReLU at the output layer.\n",
        "* A non-standard CNN model with Tanh at the output layer.\n",
        "\n",
        "Answer the following questions. Your answers must be supported by the **computation results** of your codes and the relevant concepts.\n",
        "\n",
        "1. For each model above, discuss the impact on the output distribution and model performance. (3 marks)\n",
        "\n",
        "2. Should an activation function be placed at the output layer prior to SoftMax? Explain your answer. (3 marks)"
      ],
      "metadata": {
        "id": "MojA9xFAiPQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write your answers here:"
      ],
      "metadata": {
        "id": "LXp-E-TmKRzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learning objectives:\n",
        "\n",
        "1. Extracts insights from a database.\n",
        "2. Translate insights into a game plan.\n",
        "3. Understanding the architecture of a basic CNN.\n",
        "4. Interpret the training process of a neural network model.\n",
        "5. Investigate the impact of activation functions at the OUTPUT layer of a CNN model."
      ],
      "metadata": {
        "id": "cNVT8v6rKU_O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlEv0lAp2Yox"
      },
      "source": [
        "A. Exploring the dataset\n",
        "=====================\n",
        "\n",
        "Generally, when you have to deal with image, text, audio or video data,\n",
        "you can use standard python packages that load data into a numpy array.\n",
        "Then you can convert this array into a `torch.*Tensor`.\n",
        "\n",
        "-   For images, packages such as Pillow, OpenCV are useful\n",
        "-   For audio, packages such as scipy and librosa\n",
        "-   For text, either raw Python or Cython based loading, or NLTK and\n",
        "    SpaCy are useful\n",
        "\n",
        "Specifically for vision, we have created a package called `torchvision`,\n",
        "that has data loaders for common datasets such as ImageNet, CIFAR10,\n",
        "MNIST, etc. and data transformers for images, viz.,\n",
        "`torchvision.datasets` and `torch.utils.data.DataLoader`.\n",
        "\n",
        "This provides a huge convenience and avoids writing boilerplate code.\n",
        "\n",
        "For this tutorial, we will use the CIFAR10 dataset. It has the classes:\n",
        "'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
        "'ship', 'truck'. The images in CIFAR-10 are of size 3x32x32, i.e.\n",
        "3-channel color images of 32x32 pixels in size.\n",
        "\n",
        "![cifar10](https://pytorch.org/tutorials/_static/img/cifar10.png)\n",
        "\n",
        "\n",
        "\n",
        "### A-1. Load and normalize CIFAR10\n",
        "\n",
        "Using `torchvision`, it's extremely easy to load CIFAR10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5csQoQK2Yoz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalize dataset images during loading"
      ],
      "metadata": {
        "id": "iWeEf-N3zyeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing dataset images during loading is a *standard practice in deep learning*. **It converts original pixel values range from [0,1] to [-1,1].**\n",
        "\n",
        "Benefits include:\n",
        "* Centers the data around zero\n",
        "* Makes the input distribution more symmetrical.\n",
        "* Makes the data more compatible with activation functions.\n",
        "\n",
        "To normalise the image during loading:\n",
        "\n",
        "1. ToTensor(): Converts images from PIL format (0-255 range) to torch tensors with values in [0, 1]\n",
        "2. Normalize(): Applies the formula: **(x - mean) / std** to each channel\n",
        "\n",
        "Given:\n",
        "* mean = (0.5, 0.5, 0.5)\n",
        "* std = (0.5, 0.5, 0.5)\n",
        "\n",
        "Using the formula in (2), normalized_x = (x - 0.5) / 0.5.\n",
        "\n",
        "Hence, **normalized_x = 2x - 1**\n",
        "\n",
        "Original pixel values range [0, 1] now becomes [-1, 1].\n",
        "\n",
        "To display the images using Matplotlib, we need to **unnormalize** the image. That means we are solving for x.\n",
        "\n",
        "x = (normalized_x + 1) / 2\n",
        "\n",
        "Hence, x = (normalized_x / 2) +  0.5  (formula used in the code).\n",
        "\n",
        "*Let's test*: normalized_x range [-1,1], x range [0,1]. So that's correct!"
      ],
      "metadata": {
        "id": "SwfviRBzwvDj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUA8BXcT2Yo0"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Each batch has 4 samples. Hence, 12,500 (=50k/4) batches per epoch.\n",
        "# Model updates its weights once per batch, so 12,500 updates per epoch.\n",
        "batch_size = 4\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A-2. Check out the dataset"
      ],
      "metadata": {
        "id": "pS3lw6_l4Os_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "rqfvVZUQ4V-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Dataset size\n",
        "print(f\"Training set size: {len(trainset)} images\")\n",
        "print(f\"Test set size: {len(testset)} images\")\n",
        "print(f\"Total dataset size: {len(trainset) + len(testset)} images\")\n",
        "\n",
        "# 2. Image dimensions\n",
        "sample_image, _ = trainset[0]\n",
        "print(f\"Image dimensions: {sample_image.shape}\")\n",
        "print(f\"Image type: {sample_image.dtype}\")\n",
        "\n",
        "# 3. Class distribution\n",
        "train_labels = [label for _, label in trainset]\n",
        "label_counts = Counter(train_labels)\n",
        "\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "for class_idx, count in label_counts.items():\n",
        "    print(f\"Class {classes[class_idx]}: {count} images\")"
      ],
      "metadata": {
        "id": "NxGxaIS04Rq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzYmAiuk2Yo0"
      },
      "source": [
        "Let us show some of the training images, for fun.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nJbwQGoK3_5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "def show_image_grid(images, labels, nrow=8):  # Changed nrow to 5 for a 5x5 grid\n",
        "    img_grid = torchvision.utils.make_grid(images, nrow=nrow, padding=10)\n",
        "    img_grid = img_grid / 2 + 0.5  # unnormalize\n",
        "    npimg = img_grid.numpy()\n",
        "\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Add class labels as text\n",
        "    for i, label in enumerate(labels[:len(images)]):\n",
        "        row = i // nrow\n",
        "        col = i % nrow\n",
        "        plt.text(col * (32 + 10) + 16, row * (32 + 10) + 40,\n",
        "                 classes[label], ha='center')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Usage with 25 images\n",
        "images_list = []\n",
        "labels_list = []\n",
        "for i in range(64):  # Get 25 images\n",
        "    if i % 4 == 0:\n",
        "        dataiter = iter(trainloader)\n",
        "    img, lbl = next(dataiter)\n",
        "    images_list.append(img[0])\n",
        "    labels_list.append(lbl[0])\n",
        "\n",
        "show_image_grid(images_list, labels_list)"
      ],
      "metadata": {
        "id": "mj6y43ffvJev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A-3. Run t-SNE on raw pixel values\n",
        "When you run t-SNE on raw pixel values, each image in CIFAR-10 starts as a high-dimensional vector (**3,072 dimensions for 32×32×3 RGB images**).\n",
        "\n",
        "The t-SNE algorithm then finds a way to project these high-dimensional points onto a 2D plane while trying to preserve the relative distances between points. (Similar to the idea of PCA).\n",
        "\n",
        "The x and y axes in the resulting plot are simply the two dimensions that t-SNE creates to represent the data."
      ],
      "metadata": {
        "id": "8XIS40hU4fkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a subset of images (for speed)\n",
        "subset_size = 2000\n",
        "subset_indices = np.random.choice(len(trainset), subset_size, replace=False)\n",
        "\n",
        "# Extract raw images and labels\n",
        "images = []\n",
        "labels = []\n",
        "for idx in subset_indices:\n",
        "    img, label = trainset[idx]\n",
        "    # Flatten the image to a vector\n",
        "    images.append(img.view(-1).numpy())\n",
        "    labels.append(label)\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Apply t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "images_tsne = tsne.fit_transform(images)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(10):\n",
        "    indices = labels == i\n",
        "    plt.scatter(images_tsne[indices, 0], images_tsne[indices, 1], label=classes[i], alpha=0.6)\n",
        "plt.legend()\n",
        "plt.title('t-SNE visualization of raw image pixels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o7o2k77-4h7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A-4. Calculate the Euclidean distance between each pair of classes"
      ],
      "metadata": {
        "id": "XH2JJfE0GvqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "import pandas as pd\n",
        "\n",
        "centroids = []\n",
        "for i in range(10):\n",
        "    class_points = images_tsne[labels == i]\n",
        "    centroid = class_points.mean(axis=0)\n",
        "    centroids.append(centroid)\n",
        "\n",
        "# Calculate pairwise distances\n",
        "dist_matrix = pairwise_distances(centroids)\n",
        "\n",
        "# Create labeled DataFrame\n",
        "dist_df = pd.DataFrame(dist_matrix, index=classes, columns=classes)\n",
        "\n",
        "# Print the matrix (rounded for clarity)\n",
        "print(\"\\nPairwise class centroid distances in t-SNE space (rounded):\\n\")\n",
        "print(dist_df.to_string(formatters={col: '{:.2f}'.format for col in dist_df.columns}))"
      ],
      "metadata": {
        "id": "XhP4kuf8ELGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA8dv9zL2Yo0"
      },
      "source": [
        "# B. Define a Convolutional Neural Network\n",
        "\n",
        "\n",
        "1. The following section consists of **two identical CNN models** except one has ReLU after the final layer.\n",
        "2. You will train both models and tracks their loss curves\n",
        "Evaluates their performance on the test set\n",
        "3. You will visualise:\n",
        "  * Training loss comparison\n",
        "  * Output value distributions (showing how ReLU truncates negative values)\n",
        "  * Example outputs for specific classes with and without ReLU\n",
        "4. You will compare accuracy overall and for each class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define standard model without ReLU at final layer\n",
        "class NetStandard(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # No ReLU here\n",
        "        return x"
      ],
      "metadata": {
        "id": "jB9jPa8w4xLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model with ReLU at final layer\n",
        "class NetWithFinalReLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        #self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))  # ReLU added here\n",
        "        #x = self.leaky_relu(self.fc3(x))  # LeakyReLU applied here\n",
        "        #x = torch.tanh(self.fc3(x))  # Tanh applied here\n",
        "        return x"
      ],
      "metadata": {
        "id": "q3IxfvVq42mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(net, trainloader, epochs=2):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:\n",
        "                avg_loss = running_loss / 2000\n",
        "                losses.append(avg_loss)\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {avg_loss:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "10FsQU173xHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get example outputs\n",
        "def get_sample_outputs(net, testloader):\n",
        "    # Get a batch from the testloader\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    # Get outputs\n",
        "    outputs = net(images)\n",
        "\n",
        "    return outputs, labels"
      ],
      "metadata": {
        "id": "-hcL-qVqET0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B-1. Training the model"
      ],
      "metadata": {
        "id": "uFedYRkkSymr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train both models\n",
        "print(\"Training standard model (without final ReLU)...\")\n",
        "net_standard = NetStandard()\n",
        "losses_standard = train_model(net_standard, trainloader)\n",
        "\n",
        "print(\"\\nTraining model with final ReLU...\")\n",
        "net_with_relu = NetWithFinalReLU()\n",
        "losses_with_relu = train_model(net_with_relu, trainloader)"
      ],
      "metadata": {
        "id": "RoD691kUEotj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B-2. Save the models"
      ],
      "metadata": {
        "id": "72rir0a1LfYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_S = './cifar_net_standard.pth'\n",
        "torch.save(net_standard.state_dict(), PATH_S)\n",
        "PATH_R = './cifar_net_with_relu.pth'\n",
        "torch.save(net_with_relu.state_dict(), PATH_R)"
      ],
      "metadata": {
        "id": "hM2TwG66LS-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlJ93DAw2Yo2"
      },
      "source": [
        "Next, let\\'s load back in our saved model (note: saving and re-loading\n",
        "the model wasn\\'t necessary here, we only did it to illustrate how to do\n",
        "so):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci2cs5LN2Yo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26cb267-adec-409d-fcf2-fa7dd10e916e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "#net = net_standard()\n",
        "net_standard.load_state_dict(torch.load(PATH_S, weights_only=True))\n",
        "net_with_relu.load_state_dict(torch.load(PATH_R, weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWz2Gtny2Yo4"
      },
      "source": [
        "Okay, now let us see what the neural network thinks these examples above\n",
        "are:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
      ],
      "metadata": {
        "id": "N6YQnu53WMr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxSMDipB2Yo4"
      },
      "outputs": [],
      "source": [
        "outputs_samples = net_standard(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G93SPVWS2Yo4"
      },
      "outputs": [],
      "source": [
        "_, predicted = torch.max(outputs_samples, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C. Investigate the impact of activation functions at the OUTPUT layer of a CNN model"
      ],
      "metadata": {
        "id": "xGfyUqLxLnmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C-1. Visualize the training loss and Logit values of both models"
      ],
      "metadata": {
        "id": "cYU_SeXDkemf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get sample outputs\n",
        "outputs_standard, labels = get_sample_outputs(net_standard, testloader)\n",
        "outputs_with_relu, _ = get_sample_outputs(net_with_relu, testloader)\n",
        "\n",
        "# Convert to numpy for plotting\n",
        "outputs_standard_np = outputs_standard.detach().numpy()\n",
        "outputs_with_relu_np = outputs_with_relu.detach().numpy()\n",
        "labels_np = labels.numpy()\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot loss curves\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(losses_standard, label='Standard')\n",
        "plt.plot(losses_with_relu, label='With Final ReLU')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Mini-batches (x2000)')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot output distributions (histogram of logits)\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.hist(outputs_standard_np.flatten(), bins=50, alpha=0.5, label='Standard')\n",
        "plt.hist(outputs_with_relu_np.flatten(), bins=50, alpha=0.5, label='With Final ReLU')\n",
        "plt.title('Output Distribution Comparison')\n",
        "plt.xlabel('Logit Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "# Example comparison for specific images\n",
        "plt.subplot(2, 2, 3)\n",
        "example_idx = 0  # Select first image from batch\n",
        "# Plot the output values for this example\n",
        "standard_values = outputs_standard_np[example_idx]\n",
        "relu_values = outputs_with_relu_np[example_idx]\n",
        "true_label = labels_np[example_idx]\n",
        "\n",
        "x = np.arange(10)\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, standard_values, width, label='Standard')\n",
        "plt.bar(x + width/2, relu_values, width, label='With Final ReLU')\n",
        "plt.title(f'Outputs for Example Image (True class: {classes[true_label]})')\n",
        "plt.xticks(x, classes, rotation=45)\n",
        "plt.ylabel('Logit Value')\n",
        "plt.legend()\n",
        "\n",
        "# Add another example with a different class\n",
        "plt.subplot(2, 2, 4)\n",
        "for i in range(1, len(labels_np)):\n",
        "    if labels_np[i] != labels_np[0]:  # Find example with different class\n",
        "        example_idx = i\n",
        "        break\n",
        "else:\n",
        "    example_idx = 1  # Fallback if all same class\n",
        "\n",
        "standard_values = outputs_standard_np[example_idx]\n",
        "relu_values = outputs_with_relu_np[example_idx]\n",
        "true_label = labels_np[example_idx]\n",
        "\n",
        "plt.bar(x - width/2, standard_values, width, label='Standard')\n",
        "plt.bar(x + width/2, relu_values, width, label='With Final ReLU')\n",
        "plt.title(f'Outputs for Example Image (True class: {classes[true_label]})')\n",
        "plt.xticks(x, classes, rotation=45)\n",
        "plt.ylabel('Logit Value')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TPkTirjIEZc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get outputs from the model\n",
        "outputs = net_with_relu(images)\n",
        "print(\"Sample logits (first sample):\", outputs[0])\n",
        "print(f\"Min logit: {outputs.min().item():.4f}\")\n",
        "print(f\"Max logit: {outputs.max().item():.4f}\")\n",
        "\n",
        "# Convert to NumPy array for analysis\n",
        "outputs_np = outputs.detach().numpy().flatten()\n",
        "\n",
        "# Count total and negative logits\n",
        "total_logits = outputs_np.size\n",
        "num_negatives = np.sum(outputs_np < 0)\n",
        "\n",
        "print(f\"Total number of logit values: {total_logits}\")\n",
        "print(f\"Number of negative logit values: {num_negatives}\")\n",
        "print(f\"Percentage of negative values: {100 * num_negatives / total_logits:.2f}%\")\n"
      ],
      "metadata": {
        "id": "wdp8u1NRBzQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C-2. Evaluate the models on test data"
      ],
      "metadata": {
        "id": "Y4R_gGGekoWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate both models\n",
        "def evaluate_model(net, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test images: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate and print accuracy comparison\n",
        "print(\"\\nEvaluating standard model (without final ReLU)...\")\n",
        "accuracy_standard = evaluate_model(net_standard, testloader)\n",
        "\n",
        "print(\"\\nEvaluating model with final ReLU...\")\n",
        "accuracy_with_relu = evaluate_model(net_with_relu, testloader)\n",
        "\n",
        "print(\"\\nAccuracy Comparison:\")\n",
        "print(f\"Standard Model: {accuracy_standard:.2f}%\")\n",
        "print(f\"Model with Final ReLU: {accuracy_with_relu:.2f}%\")"
      ],
      "metadata": {
        "id": "l3xvXby8Ee_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C-3. Accuracy per class in standard CNN model (without ReLU at output layer)\n"
      ],
      "metadata": {
        "id": "Lz8-72K6I4ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# checking the CNN without ReLU at output\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net_standard(images) # checking the CNN without ReLU at output\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "JoYsI5eIG18u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C-4. Accuracy per class in CNN model with ReLU at output layer"
      ],
      "metadata": {
        "id": "0N2Z4leiJHFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# checking the CNN without ReLU at output\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net_with_relu(images) # checking the CNN with ReLU at output\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "lZB0YOtiIGlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C-5. Confusion matrix for standard CNN model (without ReLU at output layer)\n"
      ],
      "metadata": {
        "id": "v3xAhoufIxMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Assuming: net is your trained model, testloader is your DataLoader, and classes is a list of class names\n",
        "net_standard.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        outputs = net_standard(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot as heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n47lu-r6IwdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C-6.  Confusion matrix for CNN model with ReLU at output layer"
      ],
      "metadata": {
        "id": "WlZZcFXEKv1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming: net is your trained model, testloader is your DataLoader, and classes is a list of class names\n",
        "net_with_relu.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        outputs = net_with_relu(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot as heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nKG0eW1RJZgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C-7. Saliency maps using GradCam\n",
        "A saliency map highlights which regions of an image most influenced the model's classification decision. The **warmer colors (red, yellow) indicate areas of high importance**, while cooler colors (blue, green) show less influential regions.\n",
        "\n",
        "When generating saliency maps, we're **visualizing the gradient flow back** through these activation functions to the input image. Different activation functions create different gradient landscapes.\n",
        "\n",
        "Study the saliency map for the different models (standard CNN vs non-standard CNN). **What do you observe?**"
      ],
      "metadata": {
        "id": "96vg2Byzoglj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam"
      ],
      "metadata": {
        "id": "QDn5oGNfrTsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "SDurdqz4tFgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_saliency_maps(net, name):\n",
        "    \"\"\"\n",
        "    Generate and display saliency maps for a given model\n",
        "\n",
        "    Args:\n",
        "        net: The trained model\n",
        "        name: Name of the model for display purposes\n",
        "    \"\"\"\n",
        "    # Select the target layer for visualization (last convolutional layer)\n",
        "    target_layer = [net.conv2]\n",
        "\n",
        "    # Initialize GradCAM\n",
        "    cam = GradCAM(model=net, target_layers=target_layer)\n",
        "\n",
        "    # Get a batch of test images\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    # Get model predictions\n",
        "    outputs = net(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Create a figure to display images and their saliency maps\n",
        "    fig, axes = plt.subplots(4, 2, figsize=(12, 16))\n",
        "    fig.suptitle(f'Saliency Maps for {name}', fontsize=16)\n",
        "\n",
        "    for i in range(4):  # Process first 4 images of the batch\n",
        "        # Original image\n",
        "        img = images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "        img = img / 2 + 0.5  # unnormalize\n",
        "\n",
        "        # Create saliency map\n",
        "        input_tensor = images[i].unsqueeze(0)  # Add batch dimension\n",
        "        grayscale_cam = cam(input_tensor=input_tensor, targets=None)\n",
        "        grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "        # Overlay saliency map on original image\n",
        "        visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "        # Display original image\n",
        "        axes[i, 0].imshow(img)\n",
        "        axes[i, 0].set_title(f'Original: {classes[labels[i]]}')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Display saliency map\n",
        "        axes[i, 1].imshow(visualization)\n",
        "        axes[i, 1].set_title(f'Predicted: {classes[predicted[i]]}')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.95)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "n1JOuDqotZzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate saliency maps for both models\n",
        "print(\"Generating saliency maps for standard model (without final ReLU)...\")\n",
        "visualize_saliency_maps(net_standard, \"Standard Model\")\n",
        "\n",
        "print(\"\\nGenerating saliency maps for model with final ReLU...\")\n",
        "visualize_saliency_maps(net_with_relu, \"Model with Final ReLU\")\n"
      ],
      "metadata": {
        "id": "_qeOxQbTtbIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also create a direct comparison between the two models\n",
        "def compare_saliency_maps():\n",
        "    \"\"\"\n",
        "    Generate and compare saliency maps between the two models\n",
        "    \"\"\"\n",
        "    # Get a batch of test images\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    # Create a figure for comparison\n",
        "    fig, axes = plt.subplots(4, 3, figsize=(15, 16))\n",
        "    fig.suptitle('Saliency Map Comparison: Standard vs. ReLU at Output', fontsize=16)\n",
        "\n",
        "    # Target layer for visualization\n",
        "    target_layer_std = [net_standard.conv2]\n",
        "    target_layer_relu = [net_with_relu.conv2]\n",
        "\n",
        "    # Initialize GradCAM for both models\n",
        "    cam_std = GradCAM(model=net_standard, target_layers=target_layer_std)\n",
        "    cam_relu = GradCAM(model=net_with_relu, target_layers=target_layer_relu)\n",
        "\n",
        "    # Get predictions\n",
        "    outputs_std = net_standard(images)\n",
        "    outputs_relu = net_with_relu(images)\n",
        "    _, predicted_std = torch.max(outputs_std, 1)\n",
        "    _, predicted_relu = torch.max(outputs_relu, 1)\n",
        "\n",
        "    for i in range(4):  # Process first 4 images\n",
        "        # Original image\n",
        "        img = images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "        img = img / 2 + 0.5  # unnormalize\n",
        "\n",
        "        # Generate saliency maps for both models\n",
        "        input_tensor = images[i].unsqueeze(0)\n",
        "\n",
        "        # Standard model saliency\n",
        "        grayscale_cam_std = cam_std(input_tensor=input_tensor, targets=None)\n",
        "        grayscale_cam_std = grayscale_cam_std[0, :]\n",
        "        vis_std = show_cam_on_image(img, grayscale_cam_std, use_rgb=True)\n",
        "\n",
        "        # ReLU model saliency\n",
        "        grayscale_cam_relu = cam_relu(input_tensor=input_tensor, targets=None)\n",
        "        grayscale_cam_relu = grayscale_cam_relu[0, :]\n",
        "        vis_relu = show_cam_on_image(img, grayscale_cam_relu, use_rgb=True)\n",
        "\n",
        "        # Display original image\n",
        "        axes[i, 0].imshow(img)\n",
        "        axes[i, 0].set_title(f'Original: {classes[labels[i]]}')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Display standard model saliency\n",
        "        axes[i, 1].imshow(vis_std)\n",
        "        axes[i, 1].set_title(f'Standard: {classes[predicted_std[i]]}')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Display ReLU model saliency\n",
        "        axes[i, 2].imshow(vis_relu)\n",
        "        axes[i, 2].set_title(f'With ReLU: {classes[predicted_relu[i]]}')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.95)\n",
        "    plt.show()\n",
        "\n",
        "# Generate side-by-side comparison\n",
        "print(\"\\nGenerating side-by-side comparison of saliency maps...\")\n",
        "compare_saliency_maps()"
      ],
      "metadata": {
        "id": "jPRKXoOVtpl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EywYiPs22Yo5"
      },
      "outputs": [],
      "source": [
        "del dataiter"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}